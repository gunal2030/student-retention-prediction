# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lSO12SSL9Mgn3vWB3F56-5Z2Zt73DWEu
"""

# METHOD 1: SIMPLE UPLOAD (CORRECTED CODE)
from google.colab import files
import pandas as pd

print("ğŸ“± STEP 1: Click 'Choose Files' below and select your dataset from your computer")
uploaded = files.upload()

print("âœ… File uploaded successfully!")

# Check what file was uploaded
file_name = list(uploaded.keys())[0]
print(f"ğŸ“„ Uploaded file: {file_name}")

# Load the data with proper encoding
try:
    # First try normal UTF-8 encoding
    if file_name.endswith('.csv'):
        df = pd.read_csv(file_name)
    elif file_name.endswith(('.xlsx', '.xls')):
        df = pd.read_excel(file_name)

except UnicodeDecodeError:
    # If UTF-8 fails, try other encodings
    print("âš ï¸  UTF-8 failed, trying other encodings...")
    if file_name.endswith('.csv'):
        df = pd.read_csv(file_name, encoding='latin-1')  # Try latin-1 encoding
    elif file_name.endswith(('.xlsx', '.xls')):
        df = pd.read_excel(file_name)

print("\nğŸ¯ STEP 2: Let's check your data")
print("=" * 50)
print(f"ğŸ“Š Dataset shape: {df.shape}")
print(f"ğŸ“ Columns: {df.columns.tolist()}")
print("\nğŸ‘€ First 5 rows:")
print(df.head())

# Let's first check what types of data we have
print("ğŸ“‹ Data Types:")
print(df.dtypes)
print("\n" + "="*50)

# Check for categorical columns
categorical_columns = df.select_dtypes(include=['object']).columns
print(f"ğŸ”¤ Categorical columns ({len(categorical_columns)}):")
for col in categorical_columns:
    print(f"  - {col}")

print("ğŸ¯ ALL COLUMNS:")
for i, col in enumerate(df.columns):
    print(f"{i+1:2d}. {col}")

from sklearn.preprocessing import LabelEncoder
import pandas as pd

# Create a copy to work with
df_encoded = df.copy()

# Convert categorical columns to numbers
label_encoders = {}
for column in df_encoded.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df_encoded[column] = le.fit_transform(df_encoded[column].astype(str))
    label_encoders[column] = le
    print(f"âœ… Encoded {column}")

print(f"\nğŸ“Š New shape: {df_encoded.shape}")
print("\nğŸ‘€ Encoded data preview:")
print(df_encoded.head())

# STEP 7: FIND THE TARGET COLUMN FOR GRADIENT BOOSTING
print("ğŸ¯ STEP 7: Finding the target column...")
print("=" * 60)

# Look for binary columns (only 2 values - 0 and 1)
print("ğŸ” Binary columns (potential targets):")
binary_found = False

for col in df_encoded.columns:
    unique_vals = df_encoded[col].nunique()
    if unique_vals == 2:
        binary_found = True
        value_counts = df_encoded[col].value_counts()
        percentage = (value_counts[1] / len(df_encoded)) * 100

        print(f"âœ… {col}:")
        print(f"   Values: {value_counts.to_dict()}")
        print(f"   Percentage of 1s: {percentage:.2f}%")
        print()

if not binary_found:
    print("âŒ No binary columns found. Let's check columns with few categories:")
    for col in df_encoded.columns:
        unique_vals = df_encoded[col].nunique()
        if unique_vals <= 5:
            value_counts = df_encoded[col].value_counts()
            print(f"ğŸ“Š {col} ({unique_vals} values): {value_counts.to_dict()}")

# STEP 8: CREATE CHURN/VISUALIZATION CHART
import matplotlib.pyplot as plt
import seaborn as sns

print("ğŸ“Š STEP 8: Creating completion distribution chart...")
print("=" * 60)

plt.figure(figsize=(8, 6))
target_counts = df['completion_flag'].value_counts()
colors = ['red', 'green']
labels = ['Not Completed', 'Completed']

bars = plt.bar(labels, target_counts, color=colors, alpha=0.7)
plt.title('Completion Status Distribution', fontsize=14, fontweight='bold')
plt.ylabel('Count')

# Add count labels on bars
for i, (bar, count) in enumerate(zip(bars, target_counts)):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10,
            f'{count} ({count/len(df)*100:.1f}%)',
            ha='center', va='bottom', fontweight='bold')

plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()

print(f"ğŸ“ˆ Completion Flag Distribution:")
print(f"   Not Completed (0): {target_counts[0]} records")
print(f"   Completed (1): {target_counts[1]} records")
print(f"   Completion Rate: {(target_counts[1] / len(df) * 100):.2f}%")

# STEP 9 (FIXED): BUILD GRADIENT BOOSTING MODEL - HANDLE MISSING VALUES
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score
import numpy as np

print("ğŸš€ STEP 9: Building Gradient Boosting Model (Handling Missing Values)...")
print("=" * 60)

# Use 'completion_flag' as target
X = df_encoded.drop('completion_flag', axis=1)  # All features except target
y = df_encoded['completion_flag']               # Target

print(f"Features (X): {X.shape}")
print(f"Target (y): {y.shape}")

# Check for missing values
print(f"\nğŸ” Checking for missing values:")
print(f"Missing values in X: {X.isnull().sum().sum()}")
print(f"Missing values in y: {y.isnull().sum().sum()}")

# Handle missing values - Fill with median for numeric columns
X_filled = X.fillna(X.median())

print(f"âœ… Missing values handled!")
print(f"Missing values after filling: {X_filled.isnull().sum().sum()}")

# Split the data (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X_filled, y, test_size=0.2, random_state=42, stratify=y)

print(f"\nğŸ“Š Data Split:")
print(f"Training set: {X_train.shape}")
print(f"Testing set: {X_test.shape}")
print(f"Training target: {y_train.value_counts().to_dict()}")
print(f"Testing target: {y_test.value_counts().to_dict()}")

# Train Gradient Boosting Model
gb_model = GradientBoostingClassifier(
    n_estimators=100,      # Number of trees
    learning_rate=0.1,     # How much each tree contributes
    max_depth=3,           # Maximum depth of each tree
    random_state=42        # For reproducible results
)

print("\nğŸ”„ Training Gradient Boosting model...")
gb_model.fit(X_train, y_train)

# Make predictions
y_pred = gb_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"\nğŸ¯ GRADIENT BOOSTING RESULTS:")
print(f"Accuracy: {accuracy * 100:.2f}%")

# STEP 10: DETAILED EVALUATION
from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score

print("ğŸ“Š STEP 10: Detailed Model Evaluation...")
print("=" * 60)

print("ğŸ“‹ Classification Report:")
print(classification_report(y_test, y_pred, target_names=['Not Completed', 'Completed']))

# Confusion Matrix
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Not Completed', 'Completed'],
            yticklabels=['Not Completed', 'Completed'])
plt.title('Gradient Boosting - Confusion Matrix\n(100% Accuracy)', fontweight='bold')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

# Additional metrics
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

print(f"ğŸ¯ Performance Metrics:")
print(f"Accuracy:  {accuracy * 100:.2f}%")
print(f"Precision: {precision * 100:.2f}%")
print(f"Recall:    {recall * 100:.2f}%")

print(f"\nğŸ” Analysis:")
print(f"- You had {X.shape[1]} features and {len(X)} samples")
print(f"- Found and filled {912} missing values")
print(f"- Model achieved perfect prediction!")

# STEP 11: CHECK FOR DATA LEAKAGE
print("ğŸ” STEP 11: Checking for Potential Data Leakage...")
print("=" * 60)

# Check if target is perfectly predictable from any single feature
print("Checking if any single feature can perfectly predict the target:")
leakage_detected = False

for feature in X_filled.columns:
    # Check if this feature alone can predict the target
    from sklearn.tree import DecisionTreeClassifier
    simple_model = DecisionTreeClassifier(max_depth=3, random_state=42)
    simple_model.fit(X_filled[[feature]], y)
    single_feature_acc = simple_model.score(X_filled[[feature]], y)

    if single_feature_acc > 0.95:  # If any single feature gives >95% accuracy
        print(f"âš ï¸  LEAKAGE WARNING: '{feature}' predicts target with {single_feature_acc*100:.1f}% accuracy")
        leakage_detected = True

if not leakage_detected:
    print("âœ… No obvious single-feature leakage detected")
    print("ğŸ’¡ The 100% accuracy might be due to:")
    print("   - Very clean and well-separated data")
    print("   - Strong patterns in the data")
    print("   - The problem being easier than expected")

print(f"\nğŸ“Š Target variable analysis:")
print(f"Completion rate: {y.mean()*100:.1f}%")
print(f"Majority class baseline: {max(y.value_counts(normalize=True))*100:.1f}%")

# STEP 12 (FIXED): REMOVE LEAKY FEATURES AND BUILD PROPER MODEL
print("ğŸ”§ STEP 12: Building Proper Model (Removing Leaky Features)...")
print("=" * 60)

# Remove the leaky features - these are essentially the target in disguise!
leaky_features = ['status_description', 'status_code', 'stage_reached']
features_to_remove = [f for f in leaky_features if f in X_filled.columns]

print(f"ğŸš« Removing leaky features: {features_to_remove}")

# Create proper features (without leaky ones)
X_proper = X_filled.drop(features_to_remove, axis=1)

print(f"âœ… Proper features shape: {X_proper.shape}")
print(f"âœ… Remaining features: {list(X_proper.columns)}")

# Split the data properly
X_train_proper, X_test_proper, y_train, y_test = train_test_split(
    X_proper, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\nğŸ“Š Proper Data Split:")
print(f"Training set: {X_train_proper.shape}")
print(f"Testing set: {X_test_proper.shape}")

# Train Gradient Boosting on proper features
gb_model_proper = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    random_state=42
)

print("\nğŸ”„ Training proper Gradient Boosting model...")
gb_model_proper.fit(X_train_proper, y_train)

# Make predictions
y_pred_proper = gb_model_proper.predict(X_test_proper)
accuracy_proper = accuracy_score(y_test, y_pred_proper)

print(f"\nğŸ¯ PROPER GRADIENT BOOSTING RESULTS:")
print(f"Accuracy: {accuracy_proper * 100:.2f}%")

# STEP 13: EVALUATE PROPER MODEL
print("ğŸ“Š STEP 13: Evaluating Proper Model (No Data Leakage)...")
print("=" * 60)

print("ğŸ“‹ Classification Report:")
print(classification_report(y_test, y_pred_proper, target_names=['Not Completed', 'Completed']))

# Confusion Matrix
plt.figure(figsize=(8, 6))
cm_proper = confusion_matrix(y_test, y_pred_proper)
sns.heatmap(cm_proper, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Not Completed', 'Completed'],
            yticklabels=['Not Completed', 'Completed'])
plt.title('Gradient Boosting - Proper Model\n(90.32% Accuracy)', fontweight='bold')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

# Additional metrics
precision_proper = precision_score(y_test, y_pred_proper)
recall_proper = recall_score(y_test, y_pred_proper)

print(f"ğŸ¯ Performance Metrics:")
print(f"Accuracy:  {accuracy_proper * 100:.2f}%")
print(f"Precision: {precision_proper * 100:.2f}%")
print(f"Recall:    {recall_proper * 100:.2f}%")

print(f"\nâœ… This is a REAL model without data leakage!")

# STEP 14: FEATURE IMPORTANCE FOR PROPER MODEL
print("ğŸ” STEP 14: Feature Importance (Proper Model)...")
print("=" * 60)

# Get feature importance from proper model
feature_importance_proper = pd.DataFrame({
    'feature': X_proper.columns,
    'importance': gb_model_proper.feature_importances_
}).sort_values('importance', ascending=False)

print("Top 10 Most Important Features:")
for i, row in feature_importance_proper.head(10).iterrows():
    print(f"   {i+1:2d}. {row['feature']}: {row['importance']:.4f}")

# Plot feature importance
plt.figure(figsize=(12, 6))
top_features_proper = feature_importance_proper.head(10)
sns.barplot(data=top_features_proper, x='importance', y='feature', palette='viridis')
plt.title('Top 10 Most Important Features - Gradient Boosting')
plt.xlabel('Importance Score')
plt.tight_layout()
plt.show()

# STEP 15: FINAL SUMMARY
print("ğŸ“Š STEP 15: Final Summary")
print("=" * 60)

print("Model Performance Summary:")
print(f"â€¢ Gradient Boosting Accuracy: {accuracy_proper * 100:.2f}%")
print(f"â€¢ Precision: {precision_proper * 100:.2f}%")
print(f"â€¢ Recall: {recall_proper * 100:.2f}%")

print(f"\nDataset Overview:")
print(f"â€¢ Original samples: {len(df)}")
print(f"â€¢ Features used: {X_proper.shape[1]}")
print(f"â€¢ Target distribution: {y.value_counts().to_dict()}")

print(f"\nKey Steps Completed:")
print("1. Data loading and encoding")
print("2. Missing value handling")
print("3. Data leakage detection and removal")
print("4. Gradient Boosting model training")
print("5. Model evaluation and feature analysis")

# STEP 16: SAVE THE MODEL
import joblib

print("ğŸ’¾ STEP 16: Saving the Model...")
print("=" * 60)

# Save the trained model
joblib.dump(gb_model_proper, 'gradient_boosting_model.pkl')
print("Model saved as 'gradient_boosting_model.pkl'")

# Save the feature names
import json
with open('model_features.json', 'w') as f:
    json.dump(list(X_proper.columns), f)
print("Feature names saved as 'model_features.json'")

print("âœ… Model and features saved successfully")